{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Neural Netwoork(project based on Fashion Mnist).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A9vrDzpsHkD",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJa0XIQmyTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "45a1317f-c8f2-4d1d-ad50-3bccc8ce822a"
      },
      "source": [
        "!pip install tensorflow-gpu==2.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bf/c28971266ca854a64f4b26f07c4112ddd61f30b4d1f18108b954a746f8ea/tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 29kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.2.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.18.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (2.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.28.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (46.1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (2.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0) (0.4.8)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRdAEbcrdlG",
        "colab_type": "text"
      },
      "source": [
        "datetime will givee date and time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DgVxYSwrWGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist #this line will take the fashion_mnist datasets from kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jncFWoEqsaBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a30acf2-a365-4f15-91e0-09e6bd35a807"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rch6y1YksqJI",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ckX2Qpsvsd",
        "colab_type": "text"
      },
      "source": [
        "**Loading the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16vIVW10tNkj",
        "colab_type": "text"
      },
      "source": [
        "(x_train, y_train) in train set\n",
        "(x_test, y_test) in test set\n",
        "_____________________________________\n",
        "(x_train contains 60000 2d arrays containing the pixels of images)\n",
        "\n",
        "(x_test contains 10000 2d arrays containing the pixels of images)\n",
        "________________________________________\n",
        "(y_train are the classes/targets of training sets)\n",
        "\n",
        "(y_test are the classes/targets of testing sets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URHQBj3PsfhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a2c07bfc-81d7-4505-d92f-5e09feb1c6f4"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTYin-VB0ZXj",
        "colab_type": "text"
      },
      "source": [
        "# Normalizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9U-BbAUygTr",
        "colab_type": "text"
      },
      "source": [
        "Normalizing the images : because it will help in images to train faster\n",
        "________________________________________\n",
        "here we divide each pixel of the image in training and test sets by the maximum number of pixel(255).\n",
        "\n",
        "In this way each pixel will be in range[0,1], By normalizing images we make sure that our model(ANN) trains faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhq9lQmqtAju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=X_train/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQP2ImK0ucS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test=X_test/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4y_xEM41hbl",
        "colab_type": "text"
      },
      "source": [
        "as we know that X_train & X_test in 3d array in which 1 dimension will give indexes and other two will give the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iiEDtvu13-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "deeaeea3-6621-4ce5-c949-57307f51d696"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGk6d_Fp08Cj",
        "colab_type": "text"
      },
      "source": [
        "# Reshaping the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OGRNeuJ1A9A",
        "colab_type": "text"
      },
      "source": [
        "since we are building a fully connected network, we reshape the training set and the test set to be into the vector format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbvi_0E2qvZ",
        "colab_type": "text"
      },
      "source": [
        "-1 says we want to reshape all the elements in the first dimension of X_train and their indexes.\n",
        "28*28 means we ant to flatten all the pixels in horizontal format to recieve the 28*28 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAUwqL6n01l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#since each image's dimension is 28 * 28, we reshape the full dataset to [-1 (all elements), height * width]\n",
        "X_train=X_train.reshape(-1, 28*28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WeoQcka2Xej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7163818-c2b7-46af-f4e6-39a3c89012b0"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXhIX7rM2jb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test=X_test.reshape(-1, 28*28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZOw8w1i30zO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "514044cb-7d5d-4cb7-9a71-e7b2bdc230ce"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdAaihyeIMcI",
        "colab_type": "text"
      },
      "source": [
        "# Building an Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZXddy34ITzQ",
        "colab_type": "text"
      },
      "source": [
        "**Defining the model**\n",
        "Simply define an object of sequential model.\n",
        "here class is a sequential class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F3PbE2r33Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHqC0_vwKWea",
        "colab_type": "text"
      },
      "source": [
        "**Adding a first fully-connected hidden layer**\n",
        "Layer hyper-parameters:\n",
        "*   number of units/neurons: 128\n",
        "*   activation function:ReLU( rectified linear unit)\n",
        "*   input_shape(784,)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFlhX59Wa0l4",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------\n",
        "add is the method, and class is the dense call, units are number of neurons, input_shape is contains the input layer/vector that is fed into neural network and input_vector contains 784 pixels that were flattened from the 2D arrays from the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kYBRCKYI1_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784, )))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjujbtemZCQR",
        "colab_type": "text"
      },
      "source": [
        "# Adding a second layer with Dropout\n",
        "Dropout is a regularization technique where we randomly set neurons in a layer to zero, that way while training process is long & we have less chance for overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abRLCc-tbxUT",
        "colab_type": "text"
      },
      "source": [
        "_________________________________________\n",
        "in other words there will be some neurons that are gonna be deactivated, therefore not learning during back propagation so their weights are not updated.\n",
        "almost 20-50% neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ-rWUsucjB7",
        "colab_type": "text"
      },
      "source": [
        "_________________________________________\n",
        "20% are dropout here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F-BOA3Fc5fG",
        "colab_type": "text"
      },
      "source": [
        "_________________________________________\n",
        "Dropout is a class.\n",
        "we do dropout to prevent over feeding.\n",
        "so if they learn too much in training set will give poor results to the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4rttOl8YekW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA3yl9GOcxlJ",
        "colab_type": "text"
      },
      "source": [
        "# Adding the output layer\n",
        "\n",
        "\n",
        "*   units: number of classes(10 in the Fashion MNIST dataset), means how many neurons you want in your output layer.\n",
        "*   activation: softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABe_72EGcTij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVU36wYb55rj",
        "colab_type": "text"
      },
      "source": [
        "# Training the ANN\n",
        "**Compiling the model**\n",
        "\n",
        "\n",
        "*   Optimizer: ADAM(adaptive moment estimation)\n",
        "*   Loss: Sparse softmax(categorical) crossentropy\n",
        "\n",
        "_________________________________________\n",
        "Optimizers are algorithms used to change the attributes of neural network such as weight & learning rate in order to reduce the losses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH0f6Mhp-EqZ",
        "colab_type": "text"
      },
      "source": [
        "_________________________________________\n",
        "compile method used here to compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVm0xyWCviN",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________\n",
        "categorical_accuracy checks to see if the index of the maximal true value is equal to the index of the maximal predicted value.\n",
        "\n",
        "sparse_categorical_accuracy checks to see if the maximal true value is equal to the index of the maximal predicted value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiIsXJ2mDVyv",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________\n",
        "Cross entropy is a loss function, used to measure the dissimilarity between the distribution of observed class labels and the predicted probabilities of class membership. Categorical refers to the possibility of having more than two classes (instead of binary, which refers to two classes). Sparse refers to using a single integer from zero to the number of classes minus one (e.g. { 0; 1; or 2 } for a class label for a three-class problem), instead of a dense one-hot encoding of the class label (e.g. { 1,0,0; 0,1,0; or 0,0,1 } for a class label for the same three-class problem). So let's say the actual class for an example from our three-class problem is 1 (0,1,0), and our predicted probabilities for the three classes are [ 0.05, 0.80, 0.15 ]. The cross entropy for this particular prediction would be -log(0.8). A smaller predicted probability for the actual class label means we have a higher loss for this prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Q9e9yI5PKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMEFco9xDwoP",
        "colab_type": "text"
      },
      "source": [
        "summary() will give every information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5vL7Zqu_4uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "35c20a43-d5d6-450f-a07a-0f0ae3b0e1f1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8nev2aXGji5",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pODSLuiGpe7",
        "colab_type": "text"
      },
      "source": [
        "here 'fit' is the method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdC-uoy0G4hB",
        "colab_type": "text"
      },
      "source": [
        "epochs are the number of times we gonna train the full amount of images in the X_train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanCUm-rDuWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "86377ec1-aef0-4d25-cd80-5772ccab1f5a"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5094 - sparse_categorical_accuracy: 0.8169\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3766 - sparse_categorical_accuracy: 0.8625\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3389 - sparse_categorical_accuracy: 0.8755\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3188 - sparse_categorical_accuracy: 0.8826\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3024 - sparse_categorical_accuracy: 0.8869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0222e70d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQMPfMdYHqWS",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the ANN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D5LIIWeHwSI",
        "colab_type": "text"
      },
      "source": [
        "**Model evaluation & prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fn036_dJMAZ",
        "colab_type": "text"
      },
      "source": [
        "as the below cell will return two values where one is loss and another is accuracy.\n",
        "_________________________________________\n",
        "here 'evaluate' is the method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY3U3Xp0HQf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e646f57-d81d-4387-84c2-828381cfd7b6"
      },
      "source": [
        "test_loss, test_accuracy=model.evaluate(X_test, y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3526 - sparse_categorical_accuracy: 0.8712\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}